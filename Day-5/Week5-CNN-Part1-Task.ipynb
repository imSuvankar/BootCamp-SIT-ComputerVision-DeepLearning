{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"0FslCE7TeuuQ"},"source":["\n","# **BOOTCAMP on Deep Learning and Computer Vision**\n","# **@Siliguri Institute of Technology, India**\n","# Week-5 | Date: Nov 03, 2023\n","\n","## **# Convolutional Neural Networks (CNNs): Part-1**\n","\n","\n","\n","### Welcome to the 5th Lab of 42028: Deep Learning and CNN!\n","\n","In this  Lab/Tutorial session you will be implementing Convolutional Neural Network for Fashion MNIST dataset classification .\n","\n","So lets get started!\n","\n","## Tutorial:\n","Implementation of a sample CNN architecture using Keras for classfication of Fashion MNIST dataset.\n","\n","## Tasks for this week:\n","\n","1. Implementation of a CNN for Dogs and Cats classification using Keras API.\n","2. Train and test model\n"]},{"cell_type":"markdown","metadata":{"id":"rahZjYapfk0y"},"source":["### Step 1: Import required packages\n","\n","we will need tensorflow, numpy, os and keras\n"]},{"cell_type":"code","metadata":{"id":"OB0ONDu2OVEm"},"source":["import tensorflow as tf\n","import os\n","import numpy as np\n","import math, numpy as np\n","import sklearn.datasets\n","import matplotlib.pyplot as plt\n","import h5py\n","import glob\n","import cv2\n","import keras.utils as image\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g1gJI8OsOuZQ"},"source":["### Step 2: Download the Fashion Mnist dataset using keras"]},{"cell_type":"code","metadata":{"id":"QsFlFK_hOyqt"},"source":["fashionMnist=tf.keras.datasets.fashion_mnist"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KhqsWWm2O6Hf"},"source":["# Load data from fashion mnist dataset using the load_data() method.\n","(train_images, train_labels), (test_images, test_labels) = fashionMnist.load_data()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hNZ63cL7PuzM"},"source":["# Display the shapes of the training images\n","print(train_images.shape)\n","print(train_images.dtype)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FOQlTPcTkqyX"},"source":["#define the class names for the fashion mnist dataset\n","class_names = [\"T-shirt/top\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\", \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Qh2YwnckTbm"},"source":["## Display an image from the dataset\n","import matplotlib.pyplot as plt\n","plt.imshow(train_images[3])\n","print(train_labels[3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DpeklHZfQRAG"},"source":["** Note :** Scikit-learn import the Fashion MNIST dataset as a 1-D array while Keras API load the dataset in 28X28 format."]},{"cell_type":"markdown","metadata":{"id":"pVNEvf6-PQIQ"},"source":["### Step 3: Normalize the dataset and split a small part of the training set into validation set\n","\n","\n","- Validation set: first 5000 samples (total 5000 samples)\n","- Training set: 5000 to remaining (total 55000 samples)"]},{"cell_type":"code","metadata":{"id":"ny-1XI3QSbav"},"source":["## WRITE YOUR CODE HERE ## (~ 5 line of code)\n","## Hint: Using slicing to split the training to train and validation\n","\n","train_images =\n","train_images =\n","train_labels =\n","\n","valid_images =\n","valid_labels =\n","\n","test_images =\n","test_images =\n","\n","### END YOUR CODE HERE ###"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bLJRDrr-dx6i"},"source":["# Print the shapes for Train, Validation, and Test dataset.\n","print(np.shape(train_images))\n","print(np.shape(valid_images))\n","print(np.shape(test_images))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rCzF5ilprJdt"},"source":["[**Expected** Output]\n","\n","(55000, 28, 28)\n","(5000, 28, 28)\n","(10000, 28, 28)"]},{"cell_type":"markdown","metadata":{"id":"Qp3nej13Pa0G"},"source":["### Step 4:  Design the CNN Architecture\n","\n","Design the following CNN architecture:\n","\n","<img src='http://drive.google.com/uc?export=view&id=1af-9cnzOpZzNf_02zdJB06zanaU3LrY2' alt='Conv'>\n","\n","\n","Input: $64 X 64 X 3$ image\n","\n","Activation function in CONV layer: Relu\n","\n","Kernel Size: 3 X 3\n","\n","Activation function in Output layer : softmax, 10 classes\n","\n","**Hint:** Use Conv2D(), MaxPooling2D(), Flatten(), and Dense()\n","\n"]},{"cell_type":"code","metadata":{"id":"DMYDkJhtPeeg"},"source":["## WRITE YOUR CODE HERE ## (~ 5 line)\n","model = tf.keras.models.Sequential([\n","\n","])\n","\n","## END YOUR CODE HERE ##"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A5OiZgAUT9l6"},"source":["## **Notes:**\n","* **Sequential model.** This is the simplest kind of Keras model, for neural networks which defines a SEQUENCE of layers.\n","\n","* **Flatten.** Flatten just takes that image and turns it into a 1-dimensional vector.\n","\n","* Next we add a second Dense hidden layer with 128 neurons, also using the ReLU activation function.  **Dense.** Add a layer to the neural network which is followed by activation function of ReLU. The ReLU only passes the value greater than 0 and for all other values of X it passes 0.\n","e.g. If X>0 return X, else return 0\"\n","\n","* Finally, we add a Dense output layer with 10 neurons (one per class), using the softmax activation function.\n","\n","* ** Softmax** The softmax takes a set of values and select the biggest one from the set of values."]},{"cell_type":"markdown","metadata":{"id":"fPG3kxXyPsIv"},"source":["## Step 5: Training the model"]},{"cell_type":"markdown","metadata":{"id":"hpJB0lyhqKr5"},"source":["**\"sparse_categorical_crossentropy\": **   The dataset contains sparse labels and the classes are exclusive.\n","\n","** One-hot vector encoding** This is sometime used for encoding the labels if there one target  probability per class for each instance. For example.\n","[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.] represent one-hot encoding for class 4. In such case, **\"categorical_crossentropy\"** loss is used.\n","\n","** \"sigmoid_crossentropy\"** This loss is used for binary class classification problems and also **\"sigmoid\"** activation function is used instead of Softmax.\n","\n"]},{"cell_type":"markdown","source":["### Visualize the CNN Architecture\n","\n","1. Using plot_model from keras.utils\n","\n","**Requires**: pydot, pydotplus, graphviz (Available on Google Colab already, installation not required)\n","\n","*   pip install pydot\n","*   pip install pydotplus\n","* pip install graphviz\n","\n","**Reference:** https://www.tensorflow.org/api_docs/python/tf/keras/utils/plot_model\n"],"metadata":{"id":"JlxrAro_WO7g"}},{"cell_type":"code","source":["# Using Plot_Model from Keras.Utils\n","model_img_file = 'Simple-CNN.png'\n","tf.keras.utils.plot_model(model, to_file=model_img_file,\n","                          show_shapes=True,\n","                          show_layer_activations=True,\n","                          rankdir='TB', # Options: TB, LR\n","                          show_dtype=False,\n","                          show_layer_names=False )\n","\n","# Also Try:\n","# show_shapes=True, show_layer_activations=True, show_dtype=True, show_layer_names=True"],"metadata":{"id":"r4BYsRElWMn2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Step 5: Training the model"],"metadata":{"id":"afezVWQUWXtO"}},{"cell_type":"markdown","source":["**\"sparse_categorical_crossentropy\": **   The dataset contains sparse labels and the classes are exclusive.\n","\n","** One-hot vector encoding** This is sometime used for encoding the labels if there one target  probability per class for each instance. For example.\n","[0., 0., 0., 0., 1., 0., 0., 0., 0., 0.] represent one-hot encoding for class 4. In such case, **\"categorical_crossentropy\"** loss is used.\n","\n","** \"sigmoid_crossentropy\"** This loss is used for binary class classification problems and also **\"sigmoid\"** activation function is used instead of Softmax.\n","\n"],"metadata":{"id":"EL_58GDfWUUk"}},{"cell_type":"code","metadata":{"id":"jBzPoV12PuNt"},"source":["model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","model.summary()\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###**Understanding on Number of Traninable Parameters:**\n","\n","**Please match the model summary with the below illustration**\n","\n","1. **CONV Layer 1:** 3X3 (Filter) X 64 (number of filters) + 64 (Bias term X number of filters) = **640**\n","\n","2. **POOL Layer parameter** = 0, no training parameters in POOL layer\n","3. **Flatten layer parameter** = 0, no traning parameters in Flatten layer\n","4. **Dense Layer 1: **128 X 10816 (from flatten layer) + 128 (bias term) = **1384576**\n","5. **Dense Layer 2:** 128 (From dense layer 1) X 10 + 10 (Bias term) = **1290**\n","\n","6. **Total Trainable parameters:** 640 + 1384576 + 1290 = **1386506**"],"metadata":{"id":"73lxig_-Wdam"}},{"cell_type":"code","source":["history = model.fit(train_images, train_labels, epochs=5)"],"metadata":{"id":"7zdwxwsIWh_1"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z_-DVZd9W6EW"},"source":["# Process the test images and find the accuracy\n","test_loss = model.evaluate(test_images, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["loss = history.history['loss']\n","#val_loss = history.history['val_loss']\n","\n","epochs = range(len(loss))\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","#plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()"],"metadata":{"id":"A6ptPANsWpIF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BsHuvS44ag6a"},"source":["## Step 6: Evaluation on test dataset"]},{"cell_type":"code","metadata":{"id":"E4pDW0FsUUmI"},"source":["model.evaluate(test_images, test_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cL4ddMJ-_hy-"},"source":["## Task: Image classification using Cats and Dogs Dataset."]},{"cell_type":"markdown","metadata":{"id":"dSZZuSdDtJWl"},"source":["###  Step: 1 Mount the Google Drive to access the Cats and Dogs Dataset\n","Reference: https://github.com/ardamavi/Dog-Cat-Classifier\n","\n"]},{"cell_type":"code","metadata":{"id":"wkhugCRrJUgB"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UFiaj8HACr9Q"},"source":["cd /content/gdrive/MyDrive/SIT-DL-CV-Bootcamp-2023/Week5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-mV5ocACv2j"},"source":["!unzip Cats-Dogs-dataset-64.zip\n","!ls"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6HQ8CEEmKTKv"},"source":["### Step : 2 Image Generators: (Preparing the dataset for train, validation and testing)\n","\n","In Keras  **keras.preprocessing.image.ImageDataGenerator** class  can be used to read images and extract labels from them via .flow_from_directory. The image generator can also be used for data augmentation. The image generators can used easily with Keras model that accept data generators as inputs. such as fit_generator, evaluate_generator, and predict_generator.\n"]},{"cell_type":"code","metadata":{"id":"m3IG-LSyV3SI"},"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","# All images will be rescaled by 1./255\n","train_datagen = ImageDataGenerator(rescale=1/255)\n","validation_datagen = ImageDataGenerator(rescale=1/255)\n","\n","\n","# Flow training images in batches of 32 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(\n","        '/content/gdrive/MyDrive/SIT-DL-CV-Bootcamp-2023/Week5',  # This is the source directory for training images\n","        target_size=(64, 64),  # All images will be resized to 64X64\n","        batch_size=30,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')\n","\n","# Flow training images in batches of 32 using train_datagen generator\n","validation_generator = validation_datagen.flow_from_directory(\n","        '/content/gdrive/MyDrive/SIT-DL-CV-Bootcamp-2023/Week5',  # This is the source directory for training images\n","        target_size=(64, 64),  # All images will be resized to 64X64\n","        batch_size=30,\n","        # Since we use binary_crossentropy loss, we need binary labels\n","        class_mode='binary')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"obfTS5wsLrYr"},"source":["### Step 3: Create the CNN model:\n","\n","Create the following CNN model:\n","\n","<img src='http://drive.google.com/uc?export=view&id=1A0vzhJyQnnhdwL5hQdYLmwJqyxr4f_Hj' alt='Conv'>\n","\n","\n","Input: $64 X 64 X 3$ image\n","\n","Activation function in CONV layer: Relu\n","\n","Kernel Size: 3X3\n","\n","Activation function in Output layer : sigmoid, 2 classes\n","\n","**Hint:** Use Conv2D(), MaxPooling2D(), Flatten(), and Dense()"]},{"cell_type":"code","metadata":{"id":"uAWjv88rC9F4"},"source":["## WRITE YOUR CODE HERE ## (~7 lines)\n","model1 = tf.keras.models.Sequential([\n","    # This is the first convolution\n","    tf.keras.layers.Conv2D(),\n","    tf.keras.layers.MaxPooling2D(),\n","\n","    # The second convolution\n","\n","\n","    # Flatten the results to feed into a DNN\n","\n","\n","    # 512 neuron hidden layer\n","\n","\n","    # Only 1 output neuron. It will contain a value from 0-1 where 0 for 1 class ('horses') and 1 for the other ('humans')\n","\n","    ])\n","\n","## END YOUR CODE HERE ##"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_vi2btHwGwBb"},"source":["##Print the model summary\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2T_v_VO6GxIx"},"source":["## Compile the model and add loss, optimizer and metrics\n","## WRITE YOUR CODE HERE ## (~1 line)\n","model1.compile()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1qu6vHJ6YXEi"},"source":["# Train/fit the model using the training and validation set.\n","## WRITE YOUR CODE HERE ## (~ 1 line)\n","history = model1.fit_generator(\n","      )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c0vYkTLGHeUg"},"source":["## Plot the Training and Validation loss\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(loss))\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'r', label='Training loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j4IBgYCYooGD"},"source":["### Clearing all the resources\n","\n","Terminate the kernel and free memory resources"]},{"cell_type":"code","metadata":{"id":"iLzc4iQ6mchi"},"source":["import os, signal\n","os.kill(os.getpid(), signal.SIGKILL)"],"execution_count":null,"outputs":[]}]}